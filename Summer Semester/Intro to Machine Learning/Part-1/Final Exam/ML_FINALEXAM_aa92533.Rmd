---
title: "Intro to ML Examination | Book - An Introduction to Statstical Learning - Second Edition "
author: "Aashi Aashi | aa92533"
date: "07/30/2022"
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
---

**Final Examination - Aashi Aashi (aa92533)**

# **Chapter 2: Question 10**

## **Part A** {#sec-part-a style="color: maroon"}

```{r include=FALSE, error=FALSE, warning=FALSE}
library(ISLR2) 
attach(Boston)
library(ggplot2) 
library(GGally) 
library(leaps)
```

```{r error=FALSE, warning=FALSE}
dim(Boston) 
```

**Results**

There are 506 rows in the Boston data set and 13 columns.

**Response Variable:** crim -per capita crime rate by town.

**Predictors:** a) zn proportion of residential land zoned for lots over 25,000 sq.ft.
b) indus proportion of non-retail business acres per town.
c) chas Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
d) nox nitrogen oxides concentration (parts per 10 million).
e) age proportion of owner-occupied units built prior to 1940.
f) dis weighted mean of distances to five Boston employment centres.
g) rad index of accessibility to radial highways.
h) tax full-value property-tax rate per \$10,000.
i) ptratio pupil-teacher ratio by town.
j) lstat lower status of the population (percent).
k) medv median value of owner-occupied homes in \$1000s.

## Part B {style="color:maroon"}

```{r error=FALSE, fig.align='left', fig.height=3, fig.width=5, warning=FALSE}
pairs(~crim+age+dis+tax+medv+indus+zn+nox, data=Boston, main = "Scatterplot Matrix")
```

Variable 'crim' is statistically positive correlated with variable 'age' i.e. as per capita crime rate by town increases, the proportion of owner-occupied units built prior to 1940 increases

Variable 'crim' is statistically negative correlated with variable 'dis' i.e. as per capita crime rate by town increases, the weighted mean of distances to five Boston employment centres decreases

Variable 'zn' is statistically negative correlated with variable 'indus' i.e. as proportion of residential land zoned for lots over 25,000 sq.ft increases, the proportion of non-retail business acres per town decreases as residential areas are usually built far off from the industries.

Variable 'zn' is statistically negative correlated with variable 'nox' i.e. as proportion of residential land zoned for lots over 25,000 sq.ft increases, the nitrogen oxides concentration decreases

Variable 'zn' is statistically negative correlated with variable 'lstat' i.e. as proportion of residential land zoned for lots over 25,000 sq.ft increases, as the percentage of lower status of population decreases

Variable 'indus' is statistically negative correlated with variable 'dis' i.e. as proportion of non-retail business acres per town increases, the weighted mean of distances to five Boston employment centres decreases

Variable 'nox' is statistically negative correlated with variable 'dis' i.e. as nitrogen oxides concentration increases, the weighted mean of distances to five Boston employment centres decreases

Variable 'nox' is statistically positive correlated with variable 'age' i.e. as nitrogen oxides concentration increases, the proportion of owner-occupied units built prior to 1940 increases

Variable 'dis' is statistically negative correlated with variable 'lstat' i.e. as weighted mean of distances to five Boston employment centres increases, the percentage of lower status of population decreases

Variable 'medv' is statistically negative correlated with variable 'crim' i.e. as the per capita crime rate increases the median value of the owner-occupied homes decreases.

Variable 'medv' is statistically positive correlated with variable 'rm'

## Part C {#sec-part-c style="color:maroon"}

```{r echo=FALSE , error=FALSE, warning=FALSE,fig.align='left', fig.height = 3, fig.width = 5}
plot(Boston$age, Boston$crim)
plot(Boston$dis, Boston$crim)
plot(Boston$rad, Boston$crim)
plot(Boston$tax, Boston$crim)
plot(Boston$ptratio, Boston$crim)
plot(Boston$medv, Boston$crim)
plot(Boston$zn, Boston$crim)
plot(Boston$indus, Boston$crim)
plot(Boston$chas, Boston$crim)
plot(Boston$nox, Boston$crim)
plot(Boston$rm, Boston$crim)
plot(Boston$lstat, Boston$crim)
```

**Result**-

Few Observations:

a)  The crime rate dramatically increases as the nitrogen oxide content exceeds a threshold of 0.6, indicating that regions with high crime rates typically have high nox levels.
    This might be because there isn't much regulation in these places, which leads to high nox and crime rates.

b)  The crime rate rises in tandem with the percentage of older housing units.
    This may be because locations with a higher percentage of older structures have lower building prices, which draw people with poor incomes, who may also include criminals.

c)  Plot suggests that the crime rate is high for the areas within 3 weighted mean distance to five Boston employment centers.

d)  Plot suggests a non-linear trend between criminal rate and lstat.
    As the lstat increases, crime rate increases.
    This may be due to the fact that if the lower status of population comprises of more criminals and when lstat increases, number of criminals in that locality also increase leading to more crime rate.

e)  The median home value and the crime rate have a non-linear inverse connection.
    Criminality declines sharply until medv = 25, at which point it plateaus.
    This could be because locations with higher medv have better local communities, higher-income residents, and fewer criminals as a result.
    Additionally, increased security in areas with high medv could cut crime rates

## Part D {#sec-part-d style="color:maroon"}

```{r echo=FALSE , error=FALSE, warning=FALSE,fig.align='left', fig.height = 3, fig.width = 5}
boxplot(Boston$crim)
```

**Result** - The box plot reveals that there are numerous suburbs with crime rates higher than Q3 + 1.5 IQR, indicating that these locations are outliers.

```{r echo=FALSE , error=FALSE, warning=FALSE,fig.align='left', fig.height = 3, fig.width = 5}
boxplot(Boston$tax)
```

**Result** - There are no data points in this set that are either above or below Q3 + 1.5 IQR, indicating that there are no outliers.

```{r echo=FALSE , error=FALSE, warning=FALSE,fig.align='left', fig.height = 3, fig.width = 5}
boxplot(Boston$ptratio)
```

**Result** - From the boxplot, we can see that there are 2 suburbs which have pupil-teacher ratio less than Q1-1.5IQR, suggesting these suburbs are outliers.

## Part E {#sec-part-e style="color:maroon"}

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf("There are %s suburbs that bound the Charles river.",sum(Boston$chas ==1))
```

## Part F {#sec-part-f style="color:maroon"}

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf("The median pupil-teacher ratio among the towns in this data set: %s",median(ptratio))
```

## Part G {style="color:maroon"}

```{r echo=FALSE,error=FALSE, warning=FALSE}
Boston[Boston$medv == min(Boston$medv), ]
```

**Result** - Both the houses are in Suburb with crim, indus, nox ,age, rad, tax, ptratio lying on or beyond 75th percentile

## Part H {style="color: maroon"}

**Number of records with average number of rooms per dwelling \>7**

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf("There are %s suburbs average more than 7 rooms per dwelling",sum(Boston$rm >7))
```

**Number of records with average number of rooms per dwelling \>8**

```{r echo=FALSE, error=FALSE, warning=FALSE}
Boston_greater_8<-Boston[Boston$rm > 8, ] 
sprintf("There are %s suburbs average more than 8 rooms per dwelling",nrow(Boston_greater_8))
```

**Number of records with average number of rooms per dwelling \<=8**

```{r echo=FALSE,error=FALSE, warning=FALSE}
Boston_less_8<-Boston[Boston$rm <= 8, ] 
sprintf("There are %s suburbs average less than 8 rooms per dwelling",nrow(Boston_less_8))
```

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf("The mean median value of owner-occupied homes in suburbs with average number of rooms per dwelling greater than 8 is %s",mean(Boston_greater_8$medv))
sprintf("The mean median value of owner-occupied homes in suburbs with average number of rooms per dwelling less than or equal 8 is %s",mean(Boston_less_8$medv))
```

**Result** - The mean median value of owner-occupied homes in suburbs with average number of rooms per dwelling \>8 (44.2) is more than twice than that of homes in suburbs with rooms \<=8 (21.96)

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf("The mean of number of lower stat people in suburbs with average number of rooms per dwelling greater than 8 is %s",mean(Boston_greater_8$lstat))
sprintf("The mean ofmnumber of lower stat people in suburbs with average number of rooms per dwelling less than or equal 8 is %s",mean(Boston_less_8$lstat))
```

**Result -** Suburbs with \> 8 average number of rooms per dwelling have \~4% (on average) proportion of lower status people, which is almost 3 times higher than that of homes in suburbs with rooms \<=8.
\~12.87%.
This makes sense as the medv value for such suburbs are quite high as discussed in previous point.

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf("The mean age of houses in suburbs with average number of rooms per dwelling greater than 8 is %s",mean(Boston_greater_8$age))
sprintf("The mean age of houses in suburbs with average number of rooms per dwelling less than or equal 8 is %s",mean(Boston_less_8$age))

```

**Result -** There is no significant difference in the age of houses in suburbs with average number of rooms per dwelling \>8 and that of homes in suburbs with rooms \<=8

# **Chapter 3: Question 15**

```{r include=FALSE, error=FALSE, warning=FALSE}
library(ISLR2)
attach(Boston) 
library(ggplot2)
error_linear <- vector()
error_polynomial <- vector ()
adj_rsq_linear <- vector()
adj_rsq_polynomial <- vector()
```

## Part A

#### Crime vs Zn

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.zn <- lm(crim ~ zn)
error_linear <- append(error_linear,sigma(lm.zn))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.zn)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.zn)
p <- ggplot(Boston, aes(zn,crim)) + geom_point()
p + geom_smooth(method = lm)
```

**Result -** zn explain less than 5% variance in crim (evident from the graph), 'zn' seems to have a statistically significant coefficient in predicting 'crim'.

#### Crime vs Indus

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.indus <- lm(crim ~ indus)
error_linear <- append(error_linear,sigma(lm.indus))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.indus)$adj.r.squared)

```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.indus)
p <- ggplot(Boston, aes( indus,crim)) + geom_point()
p + geom_smooth(method = lm)
```

**Result -** According to both the graph and the regression results, "indus" appears to have a statistically significant positive coefficient in predicting "crim," explaining less than 15% of the variance in crim.

#### Crime vs Chas

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.chas <- lm(crim ~ chas)
error_linear <- append(error_linear,sigma(lm.chas))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.chas)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.chas)
p <- ggplot(Boston, aes( chas,crim)) + geom_point()
# Add regression line
p + geom_smooth(method = lm)
```

**Result -** Chas does not have a statistically significant coefficient and only accounts for a small portion of the variance in the crim variable.
Additionally, we can observe from the graph that chas only accepts discrete values between 0 and 1, and the graph does not appear to show any relationship.

#### Crime vs Nox

```{r error=FALSE, warning=FALSE, include=FALSE}
lm.nox <- lm(crim ~ nox)
error_linear <- append(error_linear,sigma(lm.nox))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.nox)$adj.r.squared)

```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.nox)
p <- ggplot(Boston, aes( nox,crim)) + geom_point()
# Add regression line
p + geom_smooth(method = lm)
```

**Result -** Nox account for 17% of the variation in "crim." Additionally, the graph shows that they are positively associated with a statistically significant positive coefficient.

#### Crime vs Rm

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.rm <- lm(crim ~ rm)
error_linear <- append(error_linear,sigma(lm.rm))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.rm)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.rm)
p <- ggplot(Boston, aes( rm,crim)) + geom_point()
# Add regression line
p + geom_smooth(method = lm)
```

**Result -** 'rm' explain less than 5% variance in 'crim'.
'rm' has a negative correlation with 'crim' with a statistically significant coefficient.
From the graph we can see that as rm increases, crim rate decreases, but again, it explains very less variance in crim.

#### Crime vs Age

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.age <- lm(crim ~ age)
error_linear <- append(error_linear,sigma(lm.age))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.age)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.age)
p <- ggplot(Boston, aes( age,crim)) + geom_point()
# Add regression line
p + geom_smooth(method = lm)
```

**Result**- Age appears to have a statistically significant positive coefficient in predicting "crim," explaining around 12% of the variance in that variable.
We can observe from the graph that as age grows, crime also rises.

#### Crime vs Dis

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.dis<- lm(crim ~ dis)
error_linear <- append(error_linear,sigma(lm.dis))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.dis)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.dis)
p <- ggplot(Boston, aes( dis,crim)) + geom_point()
# Add regression line
p + geom_smooth(method = lm)
```

**Result -** "Dis" explains around 15% of the variation in "crim." Dis and Crim have a statistically significant negative coefficient negative association.
This negative tendency is shown in the graph.

#### Crime vs Rad

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.rad<- lm(crim ~ rad)
error_linear <- append(error_linear,sigma(lm.rad))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.rad)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.rad)
p <- ggplot(Boston, aes( rad,crim)) + geom_point()
# Add regression line
p + geom_smooth(method = lm)
```

**Result -** 'rad' explain \~40% variance in 'crim'.
'rad' has a positive correlation with 'crim' with a statistically significant positive coefficient.

#### Crime vs Tax

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.tax <- lm(crim ~ tax)
error_linear <- append(error_linear,sigma(lm.tax))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.tax)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.tax)
p <- ggplot(Boston, aes( tax,crim)) + geom_point()
# Add regression line
p + geom_smooth(method = lm)
```

**Result -** A positive association between tax rate and crim explains 33% of the variation.
It has a positive coefficient that is statistically significant.

#### Crime vs PTratio

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.ptratio <- lm(crim ~ ptratio)
error_linear <- append(error_linear,sigma(lm.ptratio))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.ptratio)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.ptratio) 
p <- ggplot(Boston, aes( ptratio,crim)) + geom_point()
# Add regression line
p + geom_smooth(method = lm)
```

**Result -** ptratio explains \<10% variation in crim with a positive correlation.
It has a statistically significant positive coefficient.

#### Crime vs LSTAT

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.lstat <- lm(crim ~ lstat)
error_linear <- append(error_linear,sigma(lm.lstat))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.lstat)$adj.r.squared)

```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.lstat)
p <- ggplot(Boston, aes( lstat,crim)) + geom_point()
# Add regression line
p + geom_smooth(method = lm)
```

**Result -** lstat has a positive correlation with crim with a statistically significant coefficient and explains \~20% variance in crim.

#### Crime vs Medv

```{r echo=FALSE, error=FALSE, warning=FALSE}
lm.medv <- lm(crim ~ medv)
error_linear <- append(error_linear,sigma(lm.medv))
adj_rsq_linear <-append(adj_rsq_linear,summary(lm.medv)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.medv)
p <- ggplot(Boston, aes( medv,crim)) + geom_point()
# Add regression line
p + geom_smooth(method = lm)
```

**Result -** medv has a negative correlation with crim with a statistically significant coefficient and explains \~15% variance in crim.

## **Part B** {style="color: maroon"}

```{r  echo=FALSE,error=FALSE, warning=FALSE}
lm.fit <- lm(crim ~ ., data = Boston)  
summary(lm.fit)
```

**Result** - These set of features explain \~44% variance in 'crim', and a residual standard error of 6.439 (model run on complete dataset) with 4 features having statistically significant coefficients at significance level (alpha) = 0.05

For variables- zn, dis, rad, medv , we can reject the null hypothesis H0 at significance level (alpha) = 0.05

## **Part C** {style="color:maroon"}

```{r echo=FALSE,error=FALSE, warning=FALSE,fig.align='left', fig.height = 3, fig.width = 5}
univeriate_reg <- vector("numeric",0)
univeriate_reg <- c(univeriate_reg, lm.zn$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.indus$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.chas$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.nox$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.rm$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.age$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.dis$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.rad$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.tax$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.ptratio$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.lstat$coefficient[2])
univeriate_reg <- c(univeriate_reg, lm.medv$coefficient[2])
multiple_reg <- vector("numeric", 0)
multiple_reg <- c(multiple_reg, lm.fit$coefficients)
multiple_reg <- multiple_reg[-1]
plot(univeriate_reg, multiple_reg, col = "blue",pch =19, ylab = "multiple regression coefficients",
     xlab = "Univariate Regression coefficients",
     main = "Relationship between Multiple regression \n and univariate regression coefficients")
```

**Results** -

Nearly all of the features had statistically significant coefficients for predicting "crim" in the results of univariate regression.
However, when all of the variables are taken into account, only a small number of them have statistically significant coefficients, suggesting that even when all of the variables are taken into account, only a small number can accurately predict "crime".
The only features with statistically significant coefficients were zn, dis, rad, black, and medv.

Additionally, we can observe that some coefficients that had favorable effects in univariate regression are now having adverse effects in multivariate analysis, and vice versa.
However, it's noteworthy to observe that the multivariate results do not show a statistically significant coefficient for these coefficients where we see such a dramatic change.

As depicted from the plot, the coefficient value for predictor nox has significantly changed from linear (univariate) model to the multiple regression model.
The value was positive (\~31) in the linear model and has now reduced to very high negative value (-10).
The change in coefficient value of nox is very high

Only 'zn' changes its impact from negative in univariate to positive in multivariate, but the overall deviation (-0.07 to +0.04) is still very low.

## **Part D** {style="color:maroon"}

#### Crime vs Zn (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.zn2 <- lm(crim ~ zn + I(zn^2)+ I(zn^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.zn2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.zn2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.zn2)
```

#### Crime vs Indus (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.indus2 <- lm(crim ~ indus + I(indus^2)+ I(indus^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.indus2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.indus2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.indus2)
```

#### Crime vs Chas (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.chas2 <- lm(crim ~ chas + I(chas^2)+ I(chas^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.chas2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.chas2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.chas2)
```

#### Crime vs Nox (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.nox2 <- lm(crim ~ nox + I(nox^2)+ I(nox^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.nox2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.nox2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.nox2)
```

#### Crime vs RM (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.rm2 <- lm(crim ~ rm + I(rm^2)+ I(rm^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.rm2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.rm2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.rm2)
```

#### Crime vs Age (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.age2 <- lm(crim ~ age + I(age^2)+ I(age^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.age2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.age2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.age2)
```

#### Crime vs Dis (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.dis2 <- lm(crim ~ dis + I(dis^2)+ I(dis^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.dis2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.dis2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.dis2)
```

#### Crime vs Rad (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.rad2 <- lm(crim ~ rad + I(rad^2)+ I(rad^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.rad2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.rad2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.rad2)
```

#### Crime vs Tax (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.tax2 <- lm(crim ~ tax + I(tax^2)+ I(tax^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.tax2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.tax2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.tax2)
```

#### Crime vs PTRATIO (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.ptratio2 <- lm(crim ~ ptratio + I(ptratio^2)+ I(ptratio^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.ptratio2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.ptratio2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.ptratio2)
```

#### Crime vs LSTAT (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.lstat2 <- lm(crim ~ lstat + I(lstat^2)+ I(lstat^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.lstat2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.lstat2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.lstat2)
```

#### Crime vs Medv (Polynomial Fit)

```{r include=FALSE, error=FALSE, warning=FALSE}
lm.medv2 <- lm(crim ~ medv + I(medv^2)+ I(medv^3)) 
error_polynomial<- append(error_polynomial,sigma(lm.medv2))
adj_rsq_polynomial <-append(adj_rsq_polynomial,summary(lm.medv2)$adj.r.squared)
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.medv2)
```

```{r echo=FALSE, error=FALSE, warning=FALSE, include=FALSE}
columns<-c("zn",  "indus"  , "chas" , "nox"  ,   "rm"    ,  "age"     ,"dis"   ,  "rad"     ,"tax"   ,  "ptratio" ,"lstat"  , "medv")
Linear_Polnyomial_comparison = data.frame(unlist(columns),unlist(error_linear),unlist(error_polynomial), unlist(adj_rsq_linear), unlist(adj_rsq_polynomial))

names(Linear_Polnyomial_comparison) = c('Features','Linear(Error)','Polynomial(Error)', 'Linear(Adj Rsq)', 'Polynomial(Adj Rsq)')
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
Linear_Polnyomial_comparison
```

**Result** - As seen from above table, Polynomial has higher Adjusted R2 error for Nox,Indus, Dis,Medv, which means it fits the data-set much better

# **Chapter 6 : Question 9**

## **Part A** {style="color:maroon"}

```{r  include=FALSE,error=FALSE, warning=FALSE}
attach(College)
set.seed (1)
sample <- sample(c(TRUE, FALSE), nrow(College),replace = TRUE, prob = c(0.6,0.4)) 
train<- College[sample,]
test<- College[!sample,]
```

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf("Few records of Train")
head(train)
sprintf("Few records of Test")
head(test)
```

**Result** - Dataset has been split into Train and Test in the ration of 80-20

## **Part B - Linear Regression** {style="color:maroon"}

```{r include=FALSE,error=FALSE, warning=FALSE}
lm.college <- lm(Apps ~ ., data = train)  
y_pred=predict(lm.college,test)
RMSE_Linear<-sqrt(mean((test$Apps - y_pred) ^ 2))
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
summary(lm.college)
sprintf("RMSE for Linear Regression is %s", RMSE_Linear)
```

## **Part C - Ridge Regression** {style="color:maroon"}

```{r include=FALSE ,include=FALSE, error=FALSE, warning=FALSE}
library(glmnet)
x <- model.matrix(Apps ~ ., train)[, -1] 
y <- train$Apps
x_test <- model.matrix(Apps ~ ., test)[, -1] 
y_test <- test$Apps
cv_model_ridge <- cv.glmnet(x, y, alpha = 0)
best_lambda <- cv_model_ridge$lambda.min
```

```{r  echo=FALSE, error=FALSE, warning=FALSE}
plot(cv_model_ridge)
sprintf("Best Lambda selected by CV is %s", best_lambda)
```

```{r include=FALSE,error=FALSE, warning=FALSE}
best_model_glmnet <- glmnet(x, y, alpha = 0, lambda = best_lambda)
y_predicted <- predict(best_model_glmnet, s = best_lambda, newx =x_test )
RMSE_Ridge<-sqrt(mean((test$Apps - y_predicted) ^ 2))
```

```{r error=FALSE, warning=FALSE}
sprintf("RMSE for Ridge Regression is %s", RMSE_Ridge)
```

## **Part D - LASSO Regression** {style="color:maroon"}

```{r include=FALSE, error=FALSE, warning=FALSE}
x <- model.matrix(Apps ~ ., train)[, -1] 
y <- train$Apps
x_test <- model.matrix(Apps ~ ., test)[, -1] 
y_test <- test$Apps
cv_model_lasso <- cv.glmnet(x, y, alpha = 1)
best_lambda <- cv_model_lasso$lambda.min
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
plot(cv_model_lasso)
sprintf("Best Lambda selected by CV is %s", best_lambda)
```

```{r include=FALSE, error=FALSE, warning=FALSE}
best_model_lasso <- glmnet(x, y, alpha = 1, lambda = best_lambda)
y_predicted <- predict(best_model_lasso, s = best_lambda, newx =x_test )
RMSE_Lasso<-sqrt(mean((test$Apps - y_predicted) ^ 2))
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
sprintf("RMSE for LASSO Regression is %s ", RMSE_Lasso)
coef(best_model_lasso)
```

**Result -** There are 5 predictors for which coefficient is coming as 0 - ***Enroll, Outstate, Terminal, SF Ratio*** *and **P.Undergrad***

## **Part E - PCR** {style="color:maroon"}

```{r include=FALSE, error=FALSE, warning=FALSE}
library(pls)
set.seed (2)
pcr.fit <- pcr(Apps ~ ., data = train, scale = TRUE,validation = "CV")
pcr.pred <- predict(pcr.fit, x_test, ncomp = 5) 
RMSE_PCR<-sqrt(mean((y_test - pcr.pred) ^ 2))
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
validationplot(pcr.fit, val.type = "MSEP")
```

**Result -** As seen in the graph above, MSEP value decrease sharply till number of components=5 and post that it kind of remains constant

```{r echo=FALSE, error=FALSE, warning=FALSE}
sprintf("RMSE for PCR is %s ", RMSE_PCR)
```

## **Part F - PLS** {style="color:maroon"}

```{r include=FALSE, error=FALSE, warning=FALSE}
set.seed (1)
pls.fit <- plsr(Apps ~ ., data = train, scale = TRUE, validation = "CV")
pls.pred <- predict(pls.fit, x_test, ncomp = 5) 
RMSE_PLS<-sqrt(mean((y_test - pls.pred) ^ 2))
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
validationplot(pls.fit, val.type = "MSEP")
```

**Result -** As seen in the graph above, MSEP value decrease sharply till number of components=2 and post that it decreases at a slow pace till number of components=4, post that it remains kind of constant

```{r echo=FALSE, error=FALSE, warning=FALSE}
sprintf("RMSE for PLS is %s ", RMSE_PLS)
```

## **Part G** {style="color:maroon"}

```{r echo=FALSE, error=FALSE, warning=FALSE}
RMSE_vector<- c(RMSE_Linear,RMSE_Ridge,RMSE_Lasso,RMSE_PCR, RMSE_PLS)
barplot(RMSE_vector,
main = "RMSE for 5 Models",
xlab = "Model",
ylab = "RMSE values",
names.arg = c("Linear ","Ridge ", "LASSO ", "PCR","PLS"),
col = "darkred")
```

**Result -** Linear Regression, LASSO and PLS have comparable MSE value among the 5 models.
PCR has the worst MSE value among all.

# **Chapter 6 : Question 11**

## Part A {style="color: maroon"}

```{r  include=FALSE,error=FALSE, warning=FALSE}
set.seed (1)
sample <- sample(c(TRUE, FALSE), nrow(Boston),replace = TRUE, prob = c(0.6,0.4)) 
train_Boston<-Boston[sample,]
test_Boston<-Boston[!sample,]
x_boston <- model.matrix(crim ~ ., train_Boston)[, -1] 
y_boston<- train_Boston$crim
x_test_boston <- model.matrix(crim ~ ., test_Boston)[, -1] 
y_test_boston <- test_Boston$crim
```

## Linear Model

```{r include=FALSE,error=FALSE, warning=FALSE}
linear_model2 = lm(crim~., data = train_Boston)
test_pred <- predict(linear_model2, newdata = test_Boston)
rms_error_lm <- sqrt(mean((test_pred - y_test_boston)^2))
```

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf('Test RMSE obtained is: %s', rms_error_lm)
```

## Best-Subset Selection and Linear Model

```{r include=FALSE,error=FALSE, warning=FALSE}
library(leaps)
regfit.full=regsubsets (crim~.,Boston, nvmax=12)
sum_var = summary(regfit.full)
par(mfrow=c(2,2))
```

```{r echo=FALSE,error=FALSE, warning=FALSE}
plot(sum_var$rss ,xlab="Number of Variables ",ylab="RSS",
type="l")
plot(sum_var$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted RSq",type="l")
```

```{r include=FALSE,error=FALSE, warning=FALSE}
get_model_formula <- function(id, object, outcome){
  models <- object$which[id,-1]
  predictors <- names(which(models == TRUE))
  predictors <- paste(predictors, collapse = "+")
  as.formula(paste0(outcome, "~", predictors))
}
linear_model3 = lm(get_model_formula(9, sum_var, "crim"), data = train_Boston)
test_pred2 <- predict(linear_model3, newdata = test_Boston)
rms_error_linear <- sqrt(mean((test_pred2 - y_test_boston)^2))

```

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf('RMSE obtained for Linear Regression is %s: ',rms_error_linear)
```

### **Using PLS**

```{r include=FALSE,error=FALSE, warning=FALSE}
set.seed (1)
pls.fit <- plsr(crim ~ ., data = train_Boston, scale = TRUE,validation = "CV")
pls.pred <- predict(pls.fit, x_test_boston, ncomp = 2) 
rms_error_pls<-sqrt(mean((y_test_boston - pls.pred) ^ 2))
```

```{r error=FALSE, warning=FALSE}
validationplot(pls.fit, val.type = "MSEP")
```

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf('RMSE obtained for PLS is %s: ',rms_error_pls)
```

### **Using Lasso**

```{r include=FALSE,error=FALSE, warning=FALSE}
boston_model_lasso <- cv.glmnet(x_boston, y_boston, alpha = 1)
best_lambda <- boston_model_lasso$lambda.min
best_model_lasso <- glmnet(x_boston, y_boston,alpha = 1, lambda = best_lambda)
y_predicted <- predict(boston_model_lasso, s = best_lambda, newx =x_test_boston )
rms_error_lasso<-sqrt(mean((y_test_boston - y_predicted) ^ 2))
```

```{r error=FALSE, warning=FALSE}
plot(boston_model_lasso)
coef(best_model_lasso)
```

```{r echo=FALSE,error=FALSE, warning=FALSE}
sprintf('RMSE obtained for Lasso is %s: ',rms_error_lasso)
```

### **Using Ridge**

```{r  include=FALSE,error=FALSE, warning=FALSE}
boston_model_ridge <- cv.glmnet(x_boston, y_boston, alpha = 0)
best_lambda <- boston_model_ridge$lambda.min
best_model_ridge <- glmnet(x_boston, y_boston,alpha = 0, lambda = best_lambda)
y_predicted <- predict(best_model_ridge, s = best_lambda, newx =x_test_boston )
rms_error_ridge<-sqrt(mean((y_test_boston - y_predicted) ^ 2))
```

```{r error=FALSE, warning=FALSE}
plot(boston_model_ridge)
coef(boston_model_ridge)
```

```{r error=FALSE, warning=FALSE}
sprintf('RMSE obtained for Ridge is %s: ',rms_error_ridge)
```

## **Part B** {style="color: maroon"}

```{r echo=FALSE, error=FALSE, warning=FALSE,fig.align='left', fig.height = 3, fig.width = 5}
RMSE_vector_error<-c(rms_error_lm,rms_error_pls,rms_error_lasso,rms_error_ridge, rms_error_linear)
barplot(RMSE_vector_error,
main = "RMSE for 4 Models",
xlab = "Model",
ylab = "RMSE values",
names.arg = c("Linear ","PLS ", "LASSO ","Ridge","Linear with BS"),
col = "darkred")
```

## Part C {style="color:maroon"}

Linear Regression has the lowest RMSE value among the 4 models, Also I have tried Running Linear Regression with Best Subset Selection.
Even though RMSE is pretty relatable with and without Best Subset Selection, selecting lesser number of variables makes the model easier to fit and less complex.

Linear Regression with Best Subset Selection is the best fitted model

# **Chapter 8 : Question 8**

## **Part A - Splitting the Dataset** {style="color: maroon"}

```{r include=FALSE,error=FALSE, warning=FALSE}
attach(Carseats)
set.seed(1)
sample=sample(c(TRUE ,FALSE), nrow(Carseats),rep=TRUE, prob=c(0.7,0.3))
train=Carseats[sample, ]
test=Carseats[!sample,]
x=model.matrix(Sales ~ ., train)[, -1]
y= train$Sales
x_test= model.matrix(Sales ~ ., test)[, -1]
y_test= test$Sales
```

```{r echo=FALSE, error=FALSE, warning=FALSE}
sprintf("Two Dimension of Train is %s",dim(train))
sprintf("Two Dimension of Test is %s",dim(test))
```

## **Part B - Trees** {style="color: maroon"}

```{r include=FALSE,error=FALSE, warning=FALSE}
library(tree)
tree.Carseats <- tree(Sales ~ ., train) 
yhat_tree <- predict(tree.Carseats ,newdata = test)
```

```{r error=FALSE, warning=FALSE}
summary(tree.Carseats)
```

```{r error=FALSE, warning=FALSE}
plot(tree.Carseats)
text(tree.Carseats , pretty = 0)
```

**Result** - As visible from the plot, shelve location is the most important predictor, followed by price.

```{r echo=FALSE,error=FALSE, warning=FALSE}
plot(yhat_tree, y_test)
abline(0, 1)
tree_MSE<-mean(((y_test - yhat_tree) ^ 2))
sprintf('MSE value for the tree is : %s',tree_MSE)
```

## **Part C - Pruned Trees** {style="color: maroon"}

```{r include=FALSE,error=FALSE, warning=FALSE}
cv.Carseats <- cv.tree(tree.Carseats)
```

```{r error=FALSE, warning=FALSE}
plot(cv.Carseats$size, cv.Carseats$dev, type = "b")
```

Result - As per Cross Validation, the optimal level of tree complexity is: 6

```{r  include=FALSE,error=FALSE, warning=FALSE}
prune.Carseats<- prune.tree(tree.Carseats, best = 6) 
yhat_prunedtree <- predict(prune.Carseats ,newdata = test)
```

```{r error=FALSE, warning=FALSE}
plot(prune.Carseats)
text(prune.Carseats , pretty = 0)
```

```{r echo=FALSE,error=FALSE, warning=FALSE}
plot(yhat_prunedtree, y_test)
abline(0, 1)
prune_tree_MSE<-(mean((y_test - yhat_prunedtree)^2))
sprintf('MSE value for the pruned tree is : %s',prune_tree_MSE)
```

**Result** - No, Pruning the tree doesn't help in improving MSE

```{r include=FALSE ,error=FALSE, warning=FALSE}
library(randomForest)
library(caret)
set.seed (1)
```

## **Part D - Bagging** {style="color: maroon"}

```{r include=FALSE ,error=FALSE, warning=FALSE}
bag.car <- randomForest(formula=Sales ~ ., data =train, mtry = 10, importance = TRUE)
yhat.bag <- predict(bag.car , newdata = test)
```

```{r echo=FALSE,Error=FALSE, warning=FALSE}
bag.car
plot(yhat.bag, y_test)
abline(0, 1)
Bagging_MSE<-(mean((y_test - yhat.bag) ^ 2))
sprintf('MSE value for the Bagging is : %s',Bagging_MSE)
```

```{r error=FALSE, warning=FALSE}
importance(bag.car)
varImpPlot(bag.car)
varImp(bag.car)
```

**Result** - ShelveLoc and Price are two most important predictors for Sales

## **Part E - Random Forest** {style="color: maroon"}

```{r include=FALSE,error=FALSE, warning=FALSE}
set.seed (1)
rf.car <- randomForest(formula=Sales ~ ., data = train, mtry = 5, importance = TRUE)
yhat.rf <- predict(rf.car, newdata =test) 

```

```{r echo=FALSE,error=FALSE, warning=FALSE}
rf.car
plot(yhat.rf, y_test)
abline(0, 1)
RF_MSE<-(mean(( y_test- yhat.rf) ^ 2))
sprintf('MSE value for the Random Forest is : %s',RF_MSE)
```

```{r error=FALSE, warning=FALSE}
importance(rf.car)
varImpPlot(rf.car)
varImp(rf.car)
```

**Result** - ShelveLoc and Price are two most import predictors for Sales

#### Relationship of M in random Forest

```{r echo=FALSE,error=FALSE, warning=FALSE, echo=FALSE,fig.align='left', fig.height = 3, fig.width = 5}
a <- 1:10
mse_rf_list <-vector()
for (a1 in a)
{
  
  rf_carseats= randomForest(Sales~.,data=train, mtry=a1, importance =TRUE)
  yhat.rf = predict(rf_carseats ,newdata=test)
  mse_random_forest<- mean((yhat.rf-y_test)^2)
  mse_rf_list <- append(mse_rf_list, mse_random_forest)

}
barplot(mse_rf_list, names.arg = a, col='maroon')
```

## **Part F - BART** {style="color: maroon"}

```{r include=FALSE,echo=FALSE ,error=FALSE, warning=FALSE}
library(BART)
set.seed (1)
bartfit <- gbart(x , y , x.test = x_test)
```

```{r echo=FALSE,error=FALSE, warning=FALSE}
yhat.bart <- bartfit$yhat.test.mean
BART_MSE<-(mean((y_test - yhat.bart) ^ 2))
sprintf('MSE value for the BART is : %s',BART_MSE)
```

```{r error=FALSE, warning=FALSE}
ord <- order(bartfit$varcount.mean, decreasing = T) 
bartfit$varcount.mean[ord]
```

**Result** -In the above output, we can check how many times each variable appeared in the collection of trees.
Price and ShelveLoc are the predictors which have occurred most number of times

#### Plotting MSE values for each Model

```{r echo=FALSE,error=FALSE, warning=FALSE,fig.align='left', fig.height = 3, fig.width = 5}
MSE_vector<-c(tree_MSE,prune_tree_MSE,Bagging_MSE, RF_MSE, BART_MSE)
barplot(MSE_vector,
main = "MSE of 5 different Models",
xlab = "Model",
ylab = "MSE value",
names.arg = c("Tree","Pruned Tree","Bagging","Random Forest","BART"),
col = "darkred")
```

**Result** - BART has lowest MSE among the 5 models we used in this problem.
Pruned Tree has the highest with \~5 MSE where as BART has the least

# **Chapter 8 : Question 11**

## **Part A** {style="color:maroon"}

```{r error=FALSE, warning=FALSE, include=FALSE}
attach(Caravan)
library(gbm)
library(class)
set.seed (1)
train<- 1:1000
carvan_encoded<-Caravan
carvan_encoded$Purchase <-ifelse(carvan_encoded$Purchase=='Yes',1,0)
Caravan_train=carvan_encoded[train,]
Caravan_test=carvan_encoded[-train,]
x_caravan <- model.matrix(Purchase ~ ., Caravan_train)[, -1] 
y_caravan <- Caravan_train$Purchase
x_test <- model.matrix(Purchase ~ ., Caravan_test)[, -1] 
y_test <- Caravan_test$Purchase
```

```{r error=FALSE, warning=FALSE}
dim(Caravan_train)
dim(Caravan_test)
```

**Result** - There are 1000 records in Caravan_train and remaining 4822 records in Caravan_test

## **Part B** {style="color:maroon"}

```{r error=FALSE, warning=FALSE}
boost.Caravan <- gbm(Purchase ~ ., data = Caravan_train,distribution = "bernoulli", n.trees = 1000, interaction.depth = 4 ,shrinkage = 0.01, verbose = F)
summary(boost.Caravan)
```

```{r error=FALSE, warning=FALSE, include=FALSE}
summary(boost.Caravan)
```

**Result** - ***PPERSAUT*** seems to be the most importance feature with relative inference of 7.782, followed by \***MGODGE**\* and \***PBRAND\***

## **Part C** {style="color:maroon"}

#### Apply Boosting Modelling Technique to the train dataset

```{r error=FALSE, warning=FALSE}
yhat.boost <- predict(boost.Caravan ,newdata = Caravan_test, n.trees = 1000, type = 'response')
yhat.boost_prediction<- ifelse(yhat.boost>0.2,1,0)
Cm_boost<-table(yhat.boost_prediction,y_test)
Cm_boost
```

```{r error=FALSE, warning=FALSE}
Precision_boost=(Cm_boost[2,2]/(Cm_boost[2,1]+Cm_boost[2,2]))*100
sprintf('Percentage of the people predicted by Boosting to make a purchase do in fact make one: %s', Precision_boost)
```

#### Applying KNN Modelling Technique to the dataset

```{r echo=FALSE ,error=FALSE, warning=FALSE}
set.seed (1)
Caravanstandardized.X <- scale(Caravan[, -86])
train<- 1:1000
train.X <- Caravanstandardized.X[train, ]
test.X <- Caravanstandardized.X[-train, ]
train.Y <- Purchase[train]
test.Y <- Purchase[-train]
knn.pred <- knn(train.X, test.X, train.Y, k = 4)
```

```{r error=FALSE, warning=FALSE}
Cm_KNN=table(knn.pred, test.Y)
Cm_KNN
```

```{r error=FALSE, warning=FALSE}
Precision_KNN=(Cm_KNN[2,2]/(Cm_KNN[2,1]+Cm_KNN[2,2]))*100
sprintf('Percentage of the people predicted by KNN to make a purchase do in fact make one: %s', Precision_KNN)
```

```{r echo=FALSE,fig.align='left', fig.height = 3, fig.width = 5}
Precision_score<-c(Precision_boost,Precision_KNN)
barplot(Precision_score,
main = "Boosting vs KNN",
xlab = "Model",
ylab = "Percentage of the people predicted to make a purchase do in fact make one",
names.arg = c("Boosting","KNN"),
col = "darkred")
```

**Result** - Percentage of the people predicted by Boosting to make a purchase do in fact make one is \~15 % where as Percentage of the people predicted by KNN to make a purchase do in fact make one is \~17 %

# **Chapter 10 : Question 7**

```{r include=FALSE, error=FALSE, warning=FALSE}
library(ISLR2)
library(keras)
library(caret)
library(neuralnet)
library(pscl)
library(InformationValue)
attach(Default)
```

#### Applying Neural Net of 1 hidden Layer with 10 Neurons

```{r include=FALSE}
Default_NN<- Default
Default_NN$default<- ifelse(Default_NN$default=='Yes',1,0)
Default_NN$student<- ifelse(Default_NN$student=='Yes',1,0)

set.seed(123)
indexes<- createDataPartition(Default_NN$default, p=.85, list = F)
Default_NN$student<- factor(Default_NN$student)
x <- scale(model.matrix(default ~ . - 1, data = Default_NN)) 
y <- Default_NN$default
x_train<-x[indexes,]
x_test<-x[-indexes,]
y_train<-y[indexes]
y_test<-y[-indexes]
modnn <- keras_model_sequential() %>%
  layer_dense(units = 10, activation = "relu", input_shape = ncol(x)) %>%
  layer_dropout(rate = 0.4) %>% layer_dense(units = 1)

modnn %>% compile(loss = "mse", optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error") )

predict_default <- modnn %>% fit(
x_train,y_train, epochs = 150, batch_size = 150, validation_data = list(x_test, y_test))
```

```{r error=FALSE, warning=FALSE,fig.align='left', fig.height = 3, fig.width = 5}
plot(predict_default)
```

```{r  error=FALSE, warning=FALSE}
npred <- predict(modnn , x_test) 
mean(abs(y_test - npred))
```

```{r}
confusionMatrix(y_test, npred)
```

**Result** - Applied the NN with one hidden layer and 10 Neurons with 150 Epochs and Batch Size = 150.
As seen, RMSE value is coming as 0.046, i.e 96% of the data is being correctly classified.
One point to note here is class is highly imbalanced , which is leading the model to predict mostly 0s, which can be seen in the confusion matrix above.
As seen, Model is predicting correct 0 - 1449 times and predicting correct 1 - only 6 times

#### Applying Logistic Regression

```{r echo=FALSE, error=FALSE, warning=FALSE}
sample <- sample(c(TRUE, FALSE), nrow(Default_NN), replace=TRUE, prob=c(0.85,0.15))
train <- Default_NN[sample, ]
test <- Default_NN[-sample, ]  
x_test=test[,-1]
y_test=test[,1]
mylogit <- glm(default ~ ., data = train, family = "binomial")
options(scipen=999)
summary(mylogit)
```

##### Accessing Model fit for Logestic Regression

```{r error=FALSE ,warning=FALSE}
pscl::pR2(mylogit)["McFadden"]
```

**Result -** Value of **0.4728807** is quite high for McFadden's R^2^, which indicates that our model fits the data very well and has high predictive power.

```{r  error=FALSE ,warning=FALSE}
caret::varImp(mylogit)
```

**Result -** Higher values indicate more importance.
These results match up nicely with the p-values from the model. 

```{r echo=FALSE, warning=FALSE, include=FALSE}
y_predicted=predict(mylogit, x_test, type="response")
```

```{r warning=FALSE, error=FALSE}
#find optimal cutoff probability to use to maximize accuracy
optimalCutoff(y_test, y_predicted)[1]
```

**Result -** Any individual with a probability of defaulting of 0.437 or higher will be predicted to default, while any individual with a probability less than this number will be predicted to not default.

```{r  warning=FALSE, error=FALSE}
confusionMatrix(y_test, y_predicted)
```

## Extra Questions

# **Problem 1 : Beauty Pays**

## Part A {style="color:maroon"}

```{r  warning=FALSE, error=FALSE}
data <- read.csv("BeautyData.csv")
lm.fit <- lm(formula = data$CourseEvals~., data=data)
summary(lm.fit)
```

```{r  warning=FALSE, error=FALSE}
lm.fit <- lm(formula = data$CourseEvals~data$BeautyScore, data=data)
summary(lm.fit)
```

**Result** - According to the stated linear regression results, BeautyScore and CourseEvals have a positive connection with a statistically significant coefficient when we attempt to predict course ratings using all the characteristics.
This indicates that beauty has a direct favorable effect on CourseEvals while holding the other factors, or in this case "other determinants," constant.

## Part B {style="color:maroon"}

Dr. Hamermesh is pointing out, in my opinion, that it is very difficult or impossible to ***isolate the impact of beauty*** on students' perceptions of teachers.
In other words, as a student will always be exposed to see a teacher's appearance when being taught in person, ***it is challenging to control for the unconscious bias linked with the same***.
Perhaps we can control for other variables if we run an experiment in which pupils aren't shown a teacher's face.
But once more, we are unable to separate the influence of voice quality from any potential unconscious bias, making it "probably impossible" to resolve this problem, in Dr. Hamermesh's words.

# **Problem 2 : Housing Price Structure**

## Part A and B {style="color:maroon"}

```{r include=FALSE,echo=FALSE, warning=FALSE, error=FALSE}
MidCity_Actual <- read.csv("MidCity.csv")
MidCity<-MidCity_Actual
MidCity$Neigborhood1<- ifelse(MidCity$Nbhd==1,1,0)
MidCity$Neigborhood3<- ifelse(MidCity$Nbhd==3,1,0)
MidCity$Brick_Param<- ifelse(MidCity$Brick=='Yes',1,0)
MidCity<-MidCity[,-2]
MidCity<-MidCity[,-4]
```

```{r warning=FALSE, error=FALSE}
lm.fit <- lm(formula = MidCity$Price~., data=MidCity)
summary(lm.fit)
```

**Part A\| Result**- As we can see from the results of the linear regression, "BrickParam" has a positive association with the price of the house and a statistically significant positive coefficient.
This implies that if the house is a brick house, its price would be higher even if all other attributes remained the same.

**Part B\| Result** - Fitted the linear regression model after one-hot encoding the Nbhd column.
According to the model summary, a house has a positive association with a house if it is located in neighborhood 3 and the correlation is statistically significant.
This suggests that, assuming all other features were the same, a home in neighborhood 3 would cost, on average, \$22,264 more than a home in a neighborhood without neighborhood 3.

## Part C {style="color:maroon"}

```{r warning=FALSE, error=FALSE}
MidCity$Neigborhood3_Brick<-MidCity$Neigborhood3*MidCity$Brick_Param
MidCity<-MidCity[,-8]
lm.fit <- lm(formula = MidCity$Price~., data=MidCity)
summary(lm.fit)
```

**Result** - Fitted the linear regression model after creating interaction term Neighborhood3_Brick which is a multiplication of Neighborhood3 and Brick_Param, essentially a flag for houses which are brick and are in neighborhood 3.
According to the model summary, a house has a positive association with a Brick house if it is located in neighborhood 3 and the correlation is statistically significant.
This suggests that, assuming all other features were the same, a home in neighborhood 3 would cost, on average, \$24,424.734 more than a Non-Brick home in a neighborhood without neighborhood 3.

## Part D {style="color:maroon"}

```{r warning=FALSE, error=FALSE}
MidCity_Actual$Neigborhood1<- ifelse(MidCity_Actual$Nbhd==1,1,0)
MidCity_Actual$Neigborhood2<- ifelse(MidCity_Actual$Nbhd==2,1,0)
MidCity_Actual$Neigborhood3<- ifelse(MidCity_Actual$Nbhd==3,1,0)
MidCity_Actual$Brick_Param<- ifelse(MidCity_Actual$Brick=='Yes',1,0)
MidCity_Actual<-MidCity_Actual[,-2]
MidCity_Actual<-MidCity_Actual[,-4]

lm.fit <- lm(formula = MidCity_Actual$Price~., data=MidCity_Actual)
summary(lm.fit)
```

**Result** - Fitted the linear regression model after one-hot encoding the Nbhd column.
According to the model summary, co-relation between Neighborhood 1 with Price and Neighborhood 2 with Price is almost same.
This suggests that, assuming all other features were the same, a home in neighborhood 2 would cost, same as a home in neighborhood 2 .

# **Problem 3: What causes what?**

## Part A {style="color:maroon"}

The facts for this can be extremely confusing since while a city with a high crime rate will recruit more police officers, it's also possible that having more officers will result in a city with a lower crime rate.
Therefore, we are unable to simply add more data and perform regression.

## Part B {style="color:maroon"}

The researchers picked the high alert days at random, so they weren't always the days with the highest crime rates.
And today was chosen to study how increasing the number of police officers affects crime rates.
As a result, the "natural experiment" was successful.

Also, from table 2 - we can see that when metro ridership is constant in Model 2 of table 2, the beta value for high alert days is still low.
On high alert days the number of cops are higher thus we can conclude that higher cops can lead to lower crime rate in this case.

## Part C {style="color:maroon"}

Controlling metro use is necessary because we don't want less people on the streets during the trial, which would result in reduced crime---which wouldn't be caused by more officers, but by fewer possible victims---rather than the opposite.

## Part D {style="color:maroon"}

The interactive impact of High Alert on various districts is seen in this table.
According to the analysis's findings, High Alert X District 1 and High Alert X Other Districts both have negative coefficients, however High Alert X District 1's coefficient is only statistically significant.
Furthermore, the coefficient of High Alert X District 1 is significantly larger than that of High Alert X Other Districts, indicating that the influence of High Alert on District 1 has on lowering crime is significantly greater than that of other districts.
On a related note, the previous regression results showed that Log(midday ridership) has a statistically significant positive coefficient, which means that as midday ridership rises, crime rates rise.
This could be because there are more people on the streets, which leads to an increase in the number of victims and crimes.

# PROJECT REPORT 

## Topic - Credit Score Classification \| Group 8

My contribution in the project involves selecting the selecting the problem statement from Kaggle along with the dataset.
Our problem statement was to predict the category of Credit Score into Good, Poor and Standard.
We needed to select a modeling technique that outputs a higher recall value irrespective of the class imbalance.
Since random forest and logistic regression were not giving us the expected results, we decided to use KNN as its Acuracy was the highest on the dataset.

All of the members of team were involved in the data cleaning, along with the initial data understanding.
Together, we performed data cleaning, ie dealing with Nulls and NaNs.
We replaced the Missing values in Categorical columns by "Unknown" and Missing Values in Numerical by finding the median of the data per Occupation.

I have performed Exploratory Data Analysis and Outliers Treatment on the cleaned data set.
EDA was divided into 2 parts - Univariate and Bi-Variate Analysis.
In the Univariate Analysis, I looked for how is the spread of numerical data set and the frequency of each categorical columns.
In the Bi- variate Analysis, I looked again the same but with respect to our Target Variable,i.e Credit_Score.
After EDA, I performed Outlier Treatment.
For this, we capped and floored the Outliers at 90th and 10th Percentile, so that we do not have to lose any data ; also not creating any bias

Our dataset contains \~100K rows and \~22 columns (after performing PCA, dropping the not required columns).I have divided the dataset into 3 parts : Train Set, Validation Set and Test Set.
I have performed Random Forest Classifier on the Dataset by using Stratified K Fold Cross Validation n_splits=10.
Stratified Cross validation validation was performed to make sure that distribution of Target Class in dataset is balanced.
Using the Trained Model, I predicted Credit_Score for the Test Set and to check if the trained Model is working fine on Unknown dataset , i.e Checking to see if the model is not overfitted or underfitted.
Random Forest gave a decent accuracy on the test data , equals to 70.67%.

Apart from KNN, Random Forest and Gradient Boosting performed equally on the Dataset.
